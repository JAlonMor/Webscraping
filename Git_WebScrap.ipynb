{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Git_WebScrap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OGHh6AKYA-yLNOZk-0EodW5zhkHFsKzE",
      "authorship_tag": "ABX9TyNvHW/4LfDvbCgVxJIviyzb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAlonMor/Webscraping/blob/main/Git_WebScrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalo fake_useragent\n",
        "\n",
        "! pip install fake_useragent\n",
        "! pip install xmltojson\n",
        "! pip install utils"
      ],
      "metadata": {
        "id": "uRKXIOlyZvx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff8984c-5bea-4676-d9e3-405a05660cf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=1e991540c0ec342781bfda4c1e156e3e83644dc27978176a058cc5e2014d0887\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n",
            "Collecting xmltojson\n",
            "  Downloading xmltojson-2.0.1-py3-none-any.whl (7.7 kB)\n",
            "Collecting xmltodict<0.13.0,>=0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: xmltodict, xmltojson\n",
            "Successfully installed xmltodict-0.12.0 xmltojson-2.0.1\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clonar el resto del repositorio si no está disponible\n",
        "import os\n",
        "curr_dir = os.getcwd()\n",
        "if not os.path.exists(os.path.join(curr_dir, '../.ROOT_DIR')):\n",
        "    !git clone https://github.com/JAlonMor/Webscraping.git\n",
        "    os.chdir(os.path.join(curr_dir, 'Webscraping/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAOq0v_feAGZ",
        "outputId": "a1933b4e-a862-4a26-83a3-962a73ac08e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Webscraping'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 17 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zYtXK6oe0itf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.core.display import HTML\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from urllib import request as urllib2\n",
        "import re\n",
        "from fake_useragent import UserAgent\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "import http.client  # or http.client if you're on Python 3\n",
        "http.client._MAXHEADERS = 1000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones\n",
        "\n",
        "def check_url(pagina):\n",
        "    \n",
        "    languaje = ['fr-CH, fr', 'en-US,es', 'es-ES,es', 'en-US,en', 'de-CH', 'es']\n",
        "    languaje_header = random.choice(languaje)\n",
        "    \n",
        "    url_base = 'https://www.coches.net/segunda-mano/?MakeId=28&ModelId=8&MinYear=2014&pg=' +str(pagina)\n",
        "    \n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    ua=UserAgent()\n",
        "    hdr = {'User-Agent': ua.random,\n",
        "          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "          'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "          'Accept-Encoding': 'none',\n",
        "          'Accept-Language': languaje_header, #+ ';q=0.9,en;q=0.8'\n",
        "          'Connection': 'keep-alive'}\n",
        "\n",
        "    r = requests.get(url_base, headers=hdr)\n",
        "    html = r.content\n",
        "    status = r.status_code\n",
        "    return status, html\n"
      ],
      "metadata": {
        "id": "N2zidje8p7yh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino archivo de salida\n",
        "f_csv = '/coches_net.csv'#'coches_net.csv'\n",
        "\n",
        "\n",
        "with open(f_csv, 'w') as f:\n",
        "\n",
        "  writer = csv.writer(f)\n",
        "\n",
        "  writer.writerow(['titulo', 'precio', 'kilometros', 'año', 'combustible', 'ciudad', 'garantia', 'url'])\n",
        "\n",
        "  # Contador segun status html\n",
        "  status = 0\n",
        "  pagina = 0\n",
        "  es_ultima = False\n",
        "\n",
        "  # Genero una búsqueda en las primeras 65 páginas, con lo obtendré más de 2000 resultados\n",
        "  \n",
        "  while es_ultima == False:\n",
        "\n",
        "    pagina = pagina +1\n",
        "    \n",
        "    time.sleep(random.randrange(7,22,1))\n",
        "    url_result = check_url(pagina)\n",
        "    html_url = url_result[1]\n",
        "    status = len(html_url)\n",
        "\n",
        "    if status >= 10000:\n",
        "      \n",
        "      print('pagina ', pagina)\n",
        "        \n",
        "    \n",
        "      # Convertir a BeatifulSoup\n",
        "      soup  = BeautifulSoup(html_url,'lxml')\n",
        "\n",
        "      # Compruebo si es la última página\n",
        "      paginacion = soup.find_all('li', attrs={'class':'sui-MoleculePagination-item'})\n",
        "      paginacion\n",
        "      pagina_siguiente = paginacion[-1].find('a').get('href')\n",
        "\n",
        "      if 'null' in pagina_siguiente:\n",
        "        es_ultima == True\n",
        "        break\n",
        "      else:\n",
        "        es_ultima == False\n",
        "\n",
        "\n",
        "      scripts = soup.find_all('script')\n",
        "      script_coches = scripts[-2]\n",
        "\n",
        "      # Extraigo script\n",
        "      js = 'window = {};\\n'+script_coches.text.strip()+';\\nprocess.stdout.write(JSON.stringify(window.__INITIAL_PROPS__));'\n",
        "\n",
        "      # Exporto a archivo temporal .js y creo objeto JSON\n",
        "      with open('temp.js','w') as f:\n",
        "        f.write(js)\n",
        "\n",
        "      from subprocess import check_output\n",
        "      script_JS = check_output(['node','temp.js'])\n",
        "\n",
        "      info_coches_js = json.loads(script_JS)\n",
        "\n",
        "      # cálculo número de anuncios en cada pagina\n",
        "      items = len(info_coches_js['initialResults']['items'])\n",
        "    \n",
        "      # Itero por todos los elementos de interés del JSON\n",
        "      for item in range(0,items-1):\n",
        "\n",
        "        titulo = info_coches_js['initialResults']['items'][item]['title'] if 'title' in info_coches_js['initialResults']['items'][item] else None\n",
        "        precio = info_coches_js['initialResults']['items'][item]['price'] if 'price' in info_coches_js['initialResults']['items'][item] else None\n",
        "        kilometros = info_coches_js['initialResults']['items'][item]['km'] if 'km' in info_coches_js['initialResults']['items'][item] else None\n",
        "        año = info_coches_js['initialResults']['items'][item]['year'] if 'year' in info_coches_js['initialResults']['items'][item] else None\n",
        "        combustible = info_coches_js['initialResults']['items'][item]['fuelTypeId'] if 'fuelTypeId' in info_coches_js['initialResults']['items'][item] else None\n",
        "        ciudad = info_coches_js['initialResults']['items'][item]['location']['mainProvince'] if 'mainProvince' in info_coches_js['initialResults']['items'][item]['location'] else None\n",
        "        garantia = info_coches_js['initialResults']['items'][item]['hasWarranty'] if 'hasWarranty' in info_coches_js['initialResults']['items'][item] else None\n",
        "        url = info_coches_js['initialResults']['items'][item]['url'] if 'url' in info_coches_js['initialResults']['items'][item] else None\n",
        "\n",
        "        writer.writerow([titulo, precio, kilometros, año, combustible, ciudad, garantia, url])\n",
        "\n",
        "    else:\n",
        "\n",
        "      time.sleep(30 + random.randrange(17,200,1))\n",
        "      pagina = pagina - 1\n",
        "      es_ultima == False\n",
        "      \n",
        "    \n",
        "    \n",
        "print('última página ')\n",
        "print('FIN')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LayuFVZgiIcs",
        "outputId": "76527555-1896-4d7c-88e0-3cde44857bbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pagina  63\n",
            "pagina  64\n",
            "pagina  65\n",
            "pagina  66\n",
            "pagina  67\n",
            "última página \n",
            "FIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mD5_NOUvalkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UzgK9TGjtJ-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}